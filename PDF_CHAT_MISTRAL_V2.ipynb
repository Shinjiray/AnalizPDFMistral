{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7371632,"sourceType":"datasetVersion","datasetId":4283127},{"sourceId":7371808,"sourceType":"datasetVersion","datasetId":4283233},{"sourceId":7456687,"sourceType":"datasetVersion","datasetId":4340414},{"sourceId":7456775,"sourceType":"datasetVersion","datasetId":4340473}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":566.781733,"end_time":"2024-01-20T23:11:32.854988","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-20T23:02:06.073255","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"06142737ef964d26800a73b15ad0fa27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08823f7c72a94ea5b2c7747dfc39f959":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f61d5d13e25400fa2e260b0176f0b4a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14579ad47719424ea72f9815365d242a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1597624b9cc14adeb2065f75969f3a01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d3cdd24de67416188efb2f9905517c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fea7aa2382a4da88632bf636a4fabf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"252b1e8139ff48d4b4063bf602f598d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d0acb7971b1471a892d9ad3246eaa5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e32107d37494c44bdac229ea9e2236c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"326fd520cb4944d5a3ff2f5cb2a04c12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3acd5947f0974c7883fb02f0a780d269":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d0acb7971b1471a892d9ad3246eaa5d","placeholder":"​","style":"IPY_MODEL_1fea7aa2382a4da88632bf636a4fabf4","value":" 5.53M/5.53M [00:00&lt;00:00, 199MB/s]"}},"3fb9b8ca2db84dac8603dae350fed48c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"411e274e63d54737af06a21e42050a00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f84af2e2f7647f7adc2cc5a7f61405b","placeholder":"​","style":"IPY_MODEL_5c38d3f3b66348889952b9c98f0cf18e","value":" 129k/129k [00:00&lt;00:00, 807kB/s]"}},"441ef355aafe43b087e37d876297064f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d257d38eb9524da18c3aa5760cd8b5d1","placeholder":"​","style":"IPY_MODEL_78d55892f9f84411b4afb796e6972542","value":"label_encoder.txt: 100%"}},"444c88d417e8442aa78069ee068c1d36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44ab1605ff6c429a8829cf4aa2f691e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bf6fd6ab5a44aacb02f425a8af1f77c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"541ac23d3dd840a8b024863ced3042b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"553c0649a9ca49d8b6dcdd25ef83f4e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_441ef355aafe43b087e37d876297064f","IPY_MODEL_c6cd6f51b2864a138a51b22df027c4da","IPY_MODEL_411e274e63d54737af06a21e42050a00"],"layout":"IPY_MODEL_dcd586b5cb024384a3e6c8245d659b07"}},"5c38d3f3b66348889952b9c98f0cf18e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f84af2e2f7647f7adc2cc5a7f61405b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64371f096c904877870e4ec7f2095286":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67b52b63feb843db9904865a45840e24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_444c88d417e8442aa78069ee068c1d36","max":5534328,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8bcd59687e4a422296124017db700c6d","value":5534328}},"68e04dd5b21f4d0dbc4afa9452b5e2de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93f0a30735b14c27b800dba01b2094e4","IPY_MODEL_67b52b63feb843db9904865a45840e24","IPY_MODEL_3acd5947f0974c7883fb02f0a780d269"],"layout":"IPY_MODEL_6d2b0c5d58ae4d70b11f85a6653e9a2e"}},"6d2b0c5d58ae4d70b11f85a6653e9a2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71927307af8441049c37019d025557d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71b7cfc0f59c4229ae9777c43c47c103":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730e592641a645a89172f9dd40160601":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78d55892f9f84411b4afb796e6972542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ca225d533604e5db4259057867c9423":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71b7cfc0f59c4229ae9777c43c47c103","placeholder":"​","style":"IPY_MODEL_beb30bd99ca941308ad4dbc1b9ba692d","value":"mean_var_norm_emb.ckpt: 100%"}},"8226ffabe8094f608ac57fa2ef36c63a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87946563143b4186996d0244c63a8f1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f61d5d13e25400fa2e260b0176f0b4a","placeholder":"​","style":"IPY_MODEL_f2749bea84b24588a0ef2322c18ab3d8","value":"hyperparams.yaml: 100%"}},"8bcd59687e4a422296124017db700c6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bdce963570549488618b67930853944":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06142737ef964d26800a73b15ad0fa27","placeholder":"​","style":"IPY_MODEL_64371f096c904877870e4ec7f2095286","value":" 1.92k/1.92k [00:00&lt;00:00, 136kB/s]"}},"92c613743c124974950f5967905ff826":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93f0a30735b14c27b800dba01b2094e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bf6fd6ab5a44aacb02f425a8af1f77c","placeholder":"​","style":"IPY_MODEL_2e32107d37494c44bdac229ea9e2236c","value":"classifier.ckpt: 100%"}},"a5893239cd5a4cef878cbfc4ac233064":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b52b466d04c3413b81b0011f16df7029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"baad7e2bbee448739c8c846a5f5f9c0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e744e02e37854e70aaef23bb188d0d43","max":83316686,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92c613743c124974950f5967905ff826","value":83316686}},"bb1eca7cbd91440d904e0020a1a5d818":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87946563143b4186996d0244c63a8f1b","IPY_MODEL_ce35d9bb3ada4ed7b694c09d12e0de20","IPY_MODEL_8bdce963570549488618b67930853944"],"layout":"IPY_MODEL_08823f7c72a94ea5b2c7747dfc39f959"}},"beb30bd99ca941308ad4dbc1b9ba692d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf5c0bbf2b904a058d5d58b32ddd6af3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d3cdd24de67416188efb2f9905517c7","placeholder":"​","style":"IPY_MODEL_71927307af8441049c37019d025557d3","value":" 1.92k/1.92k [00:00&lt;00:00, 146kB/s]"}},"c4d488287984435388a01fc6ffdb3dbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44ab1605ff6c429a8829cf4aa2f691e2","placeholder":"​","style":"IPY_MODEL_252b1e8139ff48d4b4063bf602f598d9","value":"embedding_model.ckpt: 100%"}},"c6cd6f51b2864a138a51b22df027c4da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_730e592641a645a89172f9dd40160601","max":128619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_326fd520cb4944d5a3ff2f5cb2a04c12","value":128619}},"ce35d9bb3ada4ed7b694c09d12e0de20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5893239cd5a4cef878cbfc4ac233064","max":1920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_541ac23d3dd840a8b024863ced3042b6","value":1920}},"cffc76f22c7647839feca02841584745":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ca225d533604e5db4259057867c9423","IPY_MODEL_d6a871594f084d3f9583cf9ad6facc51","IPY_MODEL_bf5c0bbf2b904a058d5d58b32ddd6af3"],"layout":"IPY_MODEL_14579ad47719424ea72f9815365d242a"}},"d257d38eb9524da18c3aa5760cd8b5d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6a871594f084d3f9583cf9ad6facc51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1597624b9cc14adeb2065f75969f3a01","max":1921,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8226ffabe8094f608ac57fa2ef36c63a","value":1921}},"dcd586b5cb024384a3e6c8245d659b07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e287ea98ad3f41cd80b8804504a89d11":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e744e02e37854e70aaef23bb188d0d43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea971ca1a3b249a39c76a75ed8bec396":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4d488287984435388a01fc6ffdb3dbf","IPY_MODEL_baad7e2bbee448739c8c846a5f5f9c0f","IPY_MODEL_fb80501e13f3403397575583d8993f7f"],"layout":"IPY_MODEL_e287ea98ad3f41cd80b8804504a89d11"}},"f2749bea84b24588a0ef2322c18ab3d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb80501e13f3403397575583d8993f7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fb9b8ca2db84dac8603dae350fed48c","placeholder":"​","style":"IPY_MODEL_b52b466d04c3413b81b0011f16df7029","value":" 83.3M/83.3M [00:00&lt;00:00, 228MB/s]"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip -q\n!pip install --upgrade kaggle\n","metadata":{"id":"yj52wI-Y8V0V","papermill":{"duration":38.251664,"end_time":"2024-01-20T23:02:47.675652","exception":false,"start_time":"2024-01-20T23:02:09.423988","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:10:48.710701Z","iopub.execute_input":"2024-01-23T11:10:48.711060Z","iopub.status.idle":"2024-01-23T11:11:25.208114Z","shell.execute_reply.started":"2024-01-23T11:10:48.711029Z","shell.execute_reply":"2024-01-23T11:11:25.207155Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.2)\nCollecting kaggle\n  Downloading kaggle-1.6.3.tar.gz (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle) (2023.11.17)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.1)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.15)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.0.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.4)\nBuilding wheels for collected packages: kaggle\n  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.3-py3-none-any.whl size=111916 sha256=11bcf852805ff723b11486b4e3273ef705f6d039a78c5ec0a65c86ca8eb1c235\n  Stored in directory: /root/.cache/pip/wheels/84/d2/34/6916f5c78356670068af8c9c17d4fac1a38fbfb71777ec12fc\nSuccessfully built kaggle\nInstalling collected packages: kaggle\n  Attempting uninstall: kaggle\n    Found existing installation: kaggle 1.6.2\n    Uninstalling kaggle-1.6.2:\n      Successfully uninstalled kaggle-1.6.2\nSuccessfully installed kaggle-1.6.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install numpy==1.23.0","metadata":{"papermill":{"duration":15.985703,"end_time":"2024-01-20T23:03:03.671656","exception":false,"start_time":"2024-01-20T23:02:47.685953","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:11:25.209927Z","iopub.execute_input":"2024-01-23T11:11:25.210200Z","iopub.status.idle":"2024-01-23T11:11:40.721102Z","shell.execute_reply.started":"2024-01-23T11:11:25.210174Z","shell.execute_reply":"2024-01-23T11:11:40.720061Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting numpy==1.23.0\n  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.3\n    Uninstalling numpy-1.24.3:\n      Successfully uninstalled numpy-1.24.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.0 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.15.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Запускаем","metadata":{"id":"jhdt_UNvGQMj","papermill":{"duration":0.010504,"end_time":"2024-01-20T23:03:03.693354","exception":false,"start_time":"2024-01-20T23:03:03.682850","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install pydantic==1.8.1","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:11:40.722500Z","iopub.execute_input":"2024-01-23T11:11:40.722808Z","iopub.status.idle":"2024-01-23T11:11:53.267844Z","shell.execute_reply.started":"2024-01-23T11:11:40.722779Z","shell.execute_reply":"2024-01-23T11:11:53.266787Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting pydantic==1.8.1\n  Downloading pydantic-1.8.1-py3-none-any.whl (125 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.8.1) (4.5.0)\nInstalling collected packages: pydantic\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.12\n    Uninstalling pydantic-1.10.12:\n      Successfully uninstalled pydantic-1.10.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nconfection 0.1.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nfastapi 0.101.1 requires pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nspacy 3.7.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nthinc 8.2.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nweasel 0.3.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-1.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"\n!pip install -U openai -q\n!pip install pytelegrambotapi -q\n!pip install -U deep-translator -q\n!pip install langchain tiktoken -q\n!pip install pydub -q\n#!pip install git+https://github.com/openai/whisper.git -q\n!sudo apt install ffmpeg","metadata":{"papermill":{"duration":91.671836,"end_time":"2024-01-20T23:04:35.376010","exception":false,"start_time":"2024-01-20T23:03:03.704174","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:11:53.270259Z","iopub.execute_input":"2024-01-23T11:11:53.270573Z","iopub.status.idle":"2024-01-23T11:13:06.214371Z","shell.execute_reply.started":"2024-01-23T11:11:53.270543Z","shell.execute_reply":"2024-01-23T11:13:06.213269Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\njupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.0 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflowjs 4.15.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.0 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.3 which is incompatible.\u001b[0m\u001b[31m\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain\n!pip install torch\n!pip install sentence_transformers\n!pip install faiss-cpu\n!pip install huggingface-hub\n!pip install pypdf\n!pip -q install accelerate\n!pip install llama-cpp-python\n!pip -q install git+https://github.com/huggingface/transformers\n!pip install fpdf\n","metadata":{"id":"kz_gWlZFw9Ve","outputId":"95e51ae9-49e6-4918-a6fa-9f1460a2d8c3","papermill":{"duration":219.665511,"end_time":"2024-01-20T23:08:15.057486","exception":false,"start_time":"2024-01-20T23:04:35.391975","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:13:06.216020Z","iopub.execute_input":"2024-01-23T11:13:06.216300Z","iopub.status.idle":"2024-01-23T11:16:38.579613Z","shell.execute_reply.started":"2024-01-23T11:13:06.216272Z","shell.execute_reply":"2024-01-23T11:16:38.578633Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.14 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.14)\nRequirement already satisfied: langchain-core<0.2,>=0.1.14 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.14)\nRequirement already satisfied: langsmith<0.0.84,>=0.0.83 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.83)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.23.0)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.14->langchain) (3.7.1)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.14->langchain) (23.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.14->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.14->langchain) (1.1.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.7.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.23.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=cfedfd9fd50546ffc1486131fa8b26cafa17736c771b08de0b11c3bc29cfeade\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.7.4\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (4.7.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub) (23.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub) (2023.11.17)\nRequirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (3.17.4)\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.2.32.tar.gz (10.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.7.1)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.0)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.32-cp310-cp310-manylinux_2_35_x86_64.whl size=2396992 sha256=f22a879e31a761f1d5fed936d60eee32f286cd1723b5cef62c11a57d9e660af2\n  Stored in directory: /root/.cache/pip/wheels/fd/34/e2/592ae50a491d192807efe8a6abcb3622fb2190402252a7258f\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.32\nCollecting fpdf\n  Downloading fpdf-1.7.2.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: fpdf\n  Building wheel for fpdf (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=33133cbc7def6c25bc0dd8084f0ad85b9e30474445f5f987c6d632bf0c1f731e\n  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\nSuccessfully built fpdf\nInstalling collected packages: fpdf\nSuccessfully installed fpdf-1.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from openai import OpenAI\nimport telebot\nimport matplotlib.pyplot as plt\nfrom deep_translator import GoogleTranslator\nfrom PIL import Image\nimport io\nimport base64\nfrom langchain.chat_models import ChatOpenAI\nimport os","metadata":{"id":"UXDGLPd3s13E","papermill":{"duration":1.55068,"end_time":"2024-01-20T23:08:19.665064","exception":false,"start_time":"2024-01-20T23:08:18.114384","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:16:38.581037Z","iopub.execute_input":"2024-01-23T11:16:38.581314Z","iopub.status.idle":"2024-01-23T11:16:40.083769Z","shell.execute_reply.started":"2024-01-23T11:16:38.581287Z","shell.execute_reply":"2024-01-23T11:16:40.082973Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Запускаем ниже остальное","metadata":{"id":"bT3hnw0g4P92","papermill":{"duration":0.032417,"end_time":"2024-01-20T23:08:19.722848","exception":false,"start_time":"2024-01-20T23:08:19.690431","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:16:40.084888Z","iopub.execute_input":"2024-01-23T11:16:40.085288Z","iopub.status.idle":"2024-01-23T11:16:40.090205Z","shell.execute_reply.started":"2024-01-23T11:16:40.085262Z","shell.execute_reply":"2024-01-23T11:16:40.088984Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import LlamaCpp\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFDirectoryLoader","metadata":{"id":"oaJJtHLRw_cS","papermill":{"duration":1.70255,"end_time":"2024-01-20T23:08:21.450133","exception":false,"start_time":"2024-01-20T23:08:19.747583","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:16:40.091273Z","iopub.execute_input":"2024-01-23T11:16:40.091540Z","iopub.status.idle":"2024-01-23T11:16:41.847913Z","shell.execute_reply.started":"2024-01-23T11:16:40.091517Z","shell.execute_reply":"2024-01-23T11:16:41.847142Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"id":"NiFR9dtOxSEo","papermill":{"duration":0.024807,"end_time":"2024-01-20T23:08:21.500452","exception":false,"start_time":"2024-01-20T23:08:21.475645","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"id":"wTlXMFevfxw2","outputId":"04374cef-233b-4d67-f764-be4c7d8500d9","papermill":{"duration":0.025684,"end_time":"2024-01-20T23:08:21.551298","exception":false,"start_time":"2024-01-20T23:08:21.525614","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нужно скачать файл модели вот отсюда https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main , подойдет любая с форматом guff/ но чем больше размер тем дольше обработка. В ячейки ниже при параметрах  n_ctx=4096 и\n\n```\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\ntext_chunks = text_splitter.split_documents(data)\n```\n\n\n средняя время обработки, например  уголовного кодекса 4-7 минут(300 страниц) одного вопроса, но маленькие 1-5 страниц (пол минуты - 1.5минуты), но любой первый вопрос он будет обрабатывать медленее чем обычно\n\n1.   А при n_ctx=4096, chunk_size=10000, chunk_overlap=2000 больше часа. Я остановил спустя 1:12/ И не знаю за сколько он справится. Как интересный пример, оставлю на ночь и посмотрю. Но это уже не в проекте.\n\n","metadata":{"id":"1lspiUDE48DR","papermill":{"duration":0.0249,"end_time":"2024-01-20T23:08:21.601329","exception":false,"start_time":"2024-01-20T23:08:21.576429","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Import Model\nllm = LlamaCpp(\n    streaming = True,\n    model_path=\"/kaggle/input/mistral4gb/mistral-7b-instruct-v0.1.Q4_K_S.gguf\",\n    temperature=0.75,\n    top_p=1,\n    verbose=True,\n    n_ctx=8096,\n\n)","metadata":{"id":"8bkS3EkEwl5s","outputId":"64df9ce7-96e6-4d84-cfd9-b4bdb02fedad","papermill":{"duration":26.387734,"end_time":"2024-01-20T23:08:48.014421","exception":false,"start_time":"2024-01-20T23:08:21.626687","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:16:41.848986Z","iopub.execute_input":"2024-01-23T11:16:41.849264Z","iopub.status.idle":"2024-01-23T11:17:19.208366Z","shell.execute_reply.started":"2024-01-23T11:16:41.849240Z","shell.execute_reply":"2024-01-23T11:17:19.207371Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /kaggle/input/mistral4gb/mistral-7b-instruct-v0.1.Q4_K_S.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 14\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  217 tensors\nllama_model_loader: - type q5_K:    8 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Small\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.11 MiB\nllm_load_tensors: offloading 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/33 layers to GPU\nllm_load_tensors:        CPU buffer size =  3947.87 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 8096\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =  1012.00 MiB\nllama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\nllama_new_context_with_model: graph splits (measure): 1\nllama_new_context_with_model:        CPU compute buffer size =     8.59 MiB\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '14'}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(llm)","metadata":{"id":"ptiFa4D5C89E","outputId":"a7bd3f8d-aaa8-49f2-dbcc-795ffcefccf8","papermill":{"duration":0.034695,"end_time":"2024-01-20T23:08:48.075219","exception":false,"start_time":"2024-01-20T23:08:48.040524","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:17:19.211455Z","iopub.execute_input":"2024-01-23T11:17:19.211939Z","iopub.status.idle":"2024-01-23T11:17:19.216592Z","shell.execute_reply.started":"2024-01-23T11:17:19.211910Z","shell.execute_reply":"2024-01-23T11:17:19.215596Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[1mLlamaCpp\u001b[0m\nParams: {'model_path': '/kaggle/input/mistral4gb/mistral-7b-instruct-v0.1.Q4_K_S.gguf', 'suffix': None, 'max_tokens': 256, 'temperature': 0.75, 'top_p': 1.0, 'logprobs': None, 'echo': False, 'stop_sequences': [], 'repeat_penalty': 1.1, 'top_k': 40}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Установка библиотек для распознования голосов","metadata":{"id":"eYDOiWMBD9NH","papermill":{"duration":0.025813,"end_time":"2024-01-20T23:08:48.126889","exception":false,"start_time":"2024-01-20T23:08:48.101076","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n\nimport whisper\nimport datetime\n\nimport subprocess\n\nimport torch\nimport pyannote.audio\nfrom pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\nembedding_model = PretrainedSpeakerEmbedding(\n    \"speechbrain/spkrec-ecapa-voxceleb\",\n    device=torch.device(\"cuda\"))\n\nfrom pyannote.audio import Audio\nfrom pyannote.core import Segment\n\nimport wave\nimport contextlib\n\nfrom sklearn.cluster import AgglomerativeClustering\nimport numpy as np\n\nfrom fpdf import FPDF\n!pip install reportlab\n\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph\n\n# Регистрация пользовательского шрифта\nfrom reportlab.pdfbase import pdfmetrics\nfrom reportlab.pdfbase.ttfonts import TTFont","metadata":{"id":"KpOmTraS_yy3","outputId":"06a196f5-ca6c-4d75-f957-0405541d388d","papermill":{"duration":97.820058,"end_time":"2024-01-20T23:10:25.973098","exception":false,"start_time":"2024-01-20T23:08:48.153040","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:17:19.217774Z","iopub.execute_input":"2024-01-23T11:17:19.218118Z","iopub.status.idle":"2024-01-23T11:18:52.710825Z","shell.execute_reply.started":"2024-01-23T11:17:19.218082Z","shell.execute_reply":"2024-01-23T11:18:52.709910Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"hyperparams.yaml:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bdecc1a8b4b46e8b34448948b49ddd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"embedding_model.ckpt:   0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b21788e1c94da7829606359d35816d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mean_var_norm_emb.ckpt:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196b978ce6bb4430ab9fe67e417a8b7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"classifier.ckpt:   0%|          | 0.00/5.53M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b8921f5516461dafd65970c02cc8fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a54d1fc7f6b4c158519d7bbd6fb8270"}},"metadata":{}},{"name":"stdout","text":"Collecting reportlab\n  Downloading reportlab-4.0.9-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from reportlab) (9.5.0)\nCollecting chardet (from reportlab)\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nDownloading reportlab-4.0.9-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: chardet, reportlab\nSuccessfully installed chardet-5.2.0 reportlab-4.0.9\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Распознаем голоса и составляем диалог в формате txt и его превращаем в pdf и сохраняем в Data так же нужно установить шрифт, для корректного отображеня PDF файла. Готовый PDF будет на гит хабе, чтобы не занимать память графического процессора в колабе,можно этот шаг пропустить предварительно загрузив в data PDF файлы, который нам понадобиться при анализе текста, в гит хабе будут готовый обработанный диалог в PDF и один большой PDF файл, кодекс РФ","metadata":{"id":"kAlaUlLdEVJt","papermill":{"duration":0.026746,"end_time":"2024-01-20T23:10:26.027445","exception":false,"start_time":"2024-01-20T23:10:26.000699","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"##Дальше снизу идет код по обработке аудиофайла","metadata":{"id":"oz8l47q79ClX","papermill":{"duration":0.026794,"end_time":"2024-01-20T23:10:26.081383","exception":false,"start_time":"2024-01-20T23:10:26.054589","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# model_size: в зависимости от выбранной вами версии 'tiny', 'base', 'small', 'medium', 'large', мы получим разное качество распознавания","metadata":{"id":"adQx6iI7D3n_","papermill":{"duration":0.027345,"end_time":"2024-01-20T23:10:26.135641","exception":false,"start_time":"2024-01-20T23:10:26.108296","status":"completed"},"tags":[]}},{"cell_type":"code","source":"num_speakers = 2 #@param {type:\"integer\"}\n\nlanguage = 'any' #@param ['any', 'English']\n\nmodel_size = 'large' #@param ['tiny', 'base', 'small', 'medium', 'large']\n\n\nmodel_name = model_size\nif language == 'English' and model_size != 'large':\n  model_name += '.en'\n\nmodel = whisper.load_model(model_size)","metadata":{"id":"jp4-uvNA_x2b","papermill":{"duration":62.267452,"end_time":"2024-01-20T23:11:28.430298","exception":false,"start_time":"2024-01-20T23:10:26.162846","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:18:52.712449Z","iopub.execute_input":"2024-01-23T11:18:52.713950Z","iopub.status.idle":"2024-01-23T11:19:44.698836Z","shell.execute_reply.started":"2024-01-23T11:18:52.713903Z","shell.execute_reply":"2024-01-23T11:19:44.697997Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████████████████████████████████| 2.88G/2.88G [00:23<00:00, 132MiB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Загружаем аудио и обрабатываем\nможем этот шаг пропустить,и загрузить вручную. Так памяти будет больше для анализа","metadata":{"id":"37bnpUlMdW36","papermill":{"duration":0.051587,"end_time":"2024-01-20T23:11:28.533025","exception":false,"start_time":"2024-01-20T23:11:28.481438","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# upload audio file\n\npath = '/kaggle/input/testwav/AUD-20231127-WA0003'\nif path[-3:] != 'wav':\n  subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n  path = 'audio.wav'\n\nresult = model.transcribe(path)\nsegments = result[\"segments\"]\nwith contextlib.closing(wave.open(path,'r')) as f:\n  frames = f.getnframes()\n  rate = f.getframerate()\n  duration = frames / float(rate)\naudio = Audio()\n\ndef segment_embedding(segment):\n  start = segment[\"start\"]\n  # Whisper overshoots the end timestamp in the last segment\n  end = min(duration, segment[\"end\"])\n  clip = Segment(start, end)\n  waveform, sample_rate = audio.crop(path, clip)\n  return embedding_model(waveform[None])\n\nembeddings1 = np.zeros(shape=(len(segments), 192))\nfor i, segment in enumerate(segments):\n  embeddings1[i] = segment_embedding(segment)\n\nembeddings1 = np.nan_to_num(embeddings1)\n\nclustering = AgglomerativeClustering(num_speakers).fit(embeddings1)\nlabels = clustering.labels_\nfor i in range(len(segments)):\n  segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n\ndef time(secs):\n  return datetime.timedelta(seconds=round(secs))\n\nf = open(\"transcript.txt\", \"w\")\n\nfor (i, segment) in enumerate(segments):\n  if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n    f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n  f.write(segment[\"text\"][1:] + ' ')\nf.close()\n\n\n\n##КОнвертируем в PDF##################################################\n\n#Установи шриф(в гит хабе естғ фаЙл)\nfont_path = '/kaggle/input/dejfont/DejaVuSans.ttf'\npdfmetrics.registerFont(TTFont(\"DejaVu\", font_path))\n\n# Создание стилей для абзацев\nstyles = getSampleStyleSheet()\ncustom_style = ParagraphStyle(\n    'CustomStyle',\n    parent=styles['Normal'],\n    fontName='DejaVu',\n    fontSize=12,\n)\n\ndef create_pdf(input_file, output_file):\n    # Создание PDF-документа\n    pdf = SimpleDocTemplate(output_file, pagesize=letter)\n\n    # Открытие текстового файла в режиме чтения\n    with open(input_file, \"r\", encoding='utf-8') as file:\n        # Сборка Paragraphs для вставки в документ\n        paragraphs = []\n        for line in file:\n            paragraphs.append(Paragraph(line, custom_style))\n\n    # Добавление Paragraphs в документ\n    pdf.build(paragraphs)\n\n# Конвертация текстового файла в PDF\ninput_file = \"transcript.txt\"\noutput_file = \"/kaggle/working/transcript.pdf\"\ncreate_pdf(input_file, output_file)\n\n","metadata":{"id":"7YaWvMfH_7Mn","outputId":"202152fc-c510-4a68-8e09-1da336f64bfa","papermill":{"duration":1.014032,"end_time":"2024-01-20T23:11:29.597993","exception":true,"start_time":"2024-01-20T23:11:28.583961","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:19:44.702406Z","iopub.execute_input":"2024-01-23T11:19:44.702720Z","iopub.status.idle":"2024-01-23T11:20:10.013984Z","shell.execute_reply.started":"2024-01-23T11:19:44.702692Z","shell.execute_reply":"2024-01-23T11:20:10.013182Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\n[mp3 @ 0x595aae234540] Estimating duration from bitrate, this may be inaccurate\nInput #0, mp3, from '/kaggle/input/testwav/AUD-20231127-WA0003':\n  Duration: 00:01:18.07, start: 0.000000, bitrate: 23 kb/s\n  Stream #0:0: Audio: mp3, 16000 Hz, mono, fltp, 24 kb/s\nStream mapping:\n  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\nPress [q] to stop, [?] for help\nOutput #0, wav, to 'audio.wav':\n  Metadata:\n    ISFT            : Lavf58.76.100\n  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n    Metadata:\n      encoder         : Lavc58.134.100 pcm_s16le\nsize=    2440kB time=00:01:18.04 bitrate= 256.1kbits/s speed=1.18e+03x    \nvideo:0kB audio:2440kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.003122%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"THsrYzXJ9N4A","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Отсөда начинает Анализ текста с PDF фалов, которые лежат в папке Data :/content/sample_data/Data","metadata":{"id":"5npv-YWf9OuU","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"#Работ по тексту, все файлы PDF в папке Data(их может быть несколько) обрабатываются и по этим файлам бот находит ответы.","metadata":{"id":"UrNMSrbgJ40z","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"loader = PyPDFDirectoryLoader(\"/kaggle/working/\")\ndata = loader.load()\n\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n#Step 05: Split the Extracted Data into Text Chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=1000)\ntext_chunks = text_splitter.split_documents(data)\nlen(text_chunks)\ntext_chunks[0]\n\nvector_store = FAISS.from_documents(text_chunks, embedding=embeddings)\n\n","metadata":{"id":"A2cE1aqne1MC","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:20:10.015310Z","iopub.execute_input":"2024-01-23T11:20:10.015696Z","iopub.status.idle":"2024-01-23T11:20:14.890882Z","shell.execute_reply.started":"2024-01-23T11:20:10.015661Z","shell.execute_reply":"2024-01-23T11:20:14.889949Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c69eb506ff8042909d686dac0a11b7e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"342b242675954f99af73a9f58064d0a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87a1077d2ef04fa4bb1f9c95adf4f4e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34188bf28c8445d8b9b8384cfb722ff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"782e9e7c37844547b3cbaccfb5a70920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b473e3c29d22486ea588b6be5d997b74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"406a1d95f0334e53bab04b6dd34b32bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1989777e9094a978dc83d965c68d783"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53fe9cc214304bac85165b86e3c12be4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6af5924105c463ab6f205af31840e2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0617705998b4dc293bde65fddca48a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3207c6c27d93421ab25435676e5ebd2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cb6a3d8314d4665908f275bb044ddd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a6087b4ca647f0a7fd1b3e0f2330a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40f06ba02c214b1a96c26425a98478cb"}},"metadata":{}}]},{"cell_type":"markdown","source":"#тут без цикла,вводим запрос в переменную query","metadata":{"id":"CeGOr0am9q6h","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"\n\nqa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}))","metadata":{"id":"3QNydLoIejTs","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-23T11:20:14.892036Z","iopub.execute_input":"2024-01-23T11:20:14.892585Z","iopub.status.idle":"2024-01-23T11:20:14.898932Z","shell.execute_reply.started":"2024-01-23T11:20:14.892557Z","shell.execute_reply":"2024-01-23T11:20:14.898087Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install python-telegram-bot","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:20:14.900038Z","iopub.execute_input":"2024-01-23T11:20:14.900296Z","iopub.status.idle":"2024-01-23T11:20:29.223808Z","shell.execute_reply.started":"2024-01-23T11:20:14.900273Z","shell.execute_reply":"2024-01-23T11:20:29.222686Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting python-telegram-bot\n  Downloading python_telegram_bot-20.7-py3-none-any.whl.metadata (15 kB)\nCollecting httpx~=0.25.2 (from python-telegram-bot)\n  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.2->python-telegram-bot) (3.7.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.2->python-telegram-bot) (2023.11.17)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.2->python-telegram-bot) (1.0.2)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.2->python-telegram-bot) (3.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.2->python-telegram-bot) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx~=0.25.2->python-telegram-bot) (0.14.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx~=0.25.2->python-telegram-bot) (1.1.3)\nDownloading python_telegram_bot-20.7-py3-none-any.whl (552 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.6/552.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: httpx, python-telegram-bot\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.26.0\n    Uninstalling httpx-0.26.0:\n      Successfully uninstalled httpx-0.26.0\nSuccessfully installed httpx-0.25.2 python-telegram-bot-20.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tempfile\nimport os\nimport subprocess\nfrom pydub import AudioSegment\nimport subprocess\nimport contextlib\nimport wave\nimport numpy as np\nfrom pydub import AudioSegment\nfrom sklearn.cluster import AgglomerativeClustering\nimport datetime\nfrom reportlab.lib.pagesizes import letter\n\nfrom reportlab.pdfbase import pdfmetrics\nfrom reportlab.pdfbase.ttfonts import TTFont\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:20:29.225323Z","iopub.execute_input":"2024-01-23T11:20:29.225662Z","iopub.status.idle":"2024-01-23T11:20:29.683214Z","shell.execute_reply.started":"2024-01-23T11:20:29.225608Z","shell.execute_reply":"2024-01-23T11:20:29.682363Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pip install config","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:20:29.684213Z","iopub.execute_input":"2024-01-23T11:20:29.684544Z","iopub.status.idle":"2024-01-23T11:20:43.092721Z","shell.execute_reply.started":"2024-01-23T11:20:29.684517Z","shell.execute_reply":"2024-01-23T11:20:43.091548Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting config\n  Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\nInstalling collected packages: config\nSuccessfully installed config-0.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import telebot\nfrom telebot import types\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:20:43.094283Z","iopub.execute_input":"2024-01-23T11:20:43.094609Z","iopub.status.idle":"2024-01-23T11:20:43.099296Z","shell.execute_reply.started":"2024-01-23T11:20:43.094578Z","shell.execute_reply":"2024-01-23T11:20:43.098410Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"bot = telebot.TeleBot('6507009266:AAG06YpNTYS_9xLts7D7A_py2phVdM85BEQ')  # ваш API-ключ для Telegram Bot\n\n@bot.message_handler(commands=['start'])\ndef start(message):\n    markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n    btn1 = types.KeyboardButton(\"1. О чём был разговор?\")\n    btn2 = types.KeyboardButton(\"2. Вежливо ли общался оператор с клиентом\")\n    btn3 = types.KeyboardButton(\"3. Была ли решена проблема клиента?\")\n    btn4 = types.KeyboardButton(\"4. Была ли нецензурная лексика в диалоге?\")\n    markup.add(btn1, btn2, btn3, btn4)\n    bot.send_message(message.chat.id, text=\"Привет, {0.first_name}! Я тестовый бот \".format(message.from_user), reply_markup=markup)\n\n@bot.message_handler(content_types=['text', 'audio'])\ndef handle_messages(message):\n    if message.content_type == 'text':\n        user_input = message.text\n        bot.send_message(message.from_user.id, \"Я обрабатываю ваш запрос. Ожидайте 2-5 минут\")\n        loader = PyPDFDirectoryLoader(\"/kaggle/working/\")\n        data = loader.load()\n\n        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=1000)\n        text_chunks = text_splitter.split_documents(data)\n\n        vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)\n        qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}))\n        result = qa.run(user_input)\n        bot.send_message(message.from_user.id, result)\n    elif message.content_type == 'audio':\n        # Сохраняем аудиофайл, чтобы обработать его\n        bot.send_message(message.from_user.id, \"Файл на обработке\")\n        file_id = message.audio.file_id\n        file_info = bot.get_file(file_id)\n        downloaded_file = bot.download_file(file_info.file_path)\n        temp_file_path = \"/kaggle/working/audio_input.ogg\"  # Выберите формат, поддерживаемый pydub\n        with open(temp_file_path, 'wb') as new_file:\n            new_file.write(downloaded_file)\n\n        # Обрабатываем аудиофайл\n        result = process_audio(temp_file_path)\n        bot.send_message(message.from_user.id, result)\n\n\n# Функция для обработки аудиофайла\ndef process_audio(audio_path):\n    if audio_path[-3:] != 'wav':\n        subprocess.call(['ffmpeg', '-i', audio_path, 'audio.wav', '-y'])\n        audio_path = 'audio.wav'\n\n    result = model.transcribe(audio_path)\n    segments = result[\"segments\"]\n\n    with contextlib.closing(wave.open(audio_path, 'r')) as f:\n        frames = f.getnframes()\n        rate = f.getframerate()\n        duration = frames / float(rate)\n\n    audio = Audio()\n\n    def segment_embedding(segment):\n        start = segment[\"start\"]\n        end = min(duration, segment[\"end\"])\n        clip = Segment(start, end)\n        waveform, sample_rate = audio.crop(audio_path, clip)\n        return embedding_model(waveform[None])\n\n    embeddings1 = np.zeros(shape=(len(segments), 192))\n    for i, segment in enumerate(segments):\n        embeddings1[i] = segment_embedding(segment)\n\n    embeddings1 = np.nan_to_num(embeddings1)\n\n    clustering = AgglomerativeClustering(num_speakers).fit(embeddings1)\n    labels = clustering.labels_\n    for i in range(len(segments)):\n        segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n\n    def time(secs):\n        return datetime.timedelta(seconds=round(secs))\n\n    transcript_content = \"\"\n    for (i, segment) in enumerate(segments):\n        if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n            transcript_content += \"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n'\n        transcript_content += segment[\"text\"][1:] + ' '\n\n    transcript_file_path = \"/kaggle/working/transcript.txt\"\n    with open(transcript_file_path, \"w\") as f:\n        f.write(transcript_content)\n\n    # Конвертация текстового файла в PDF\n    font_path = '/kaggle/input/dejfont/DejaVuSans.ttf'\n    pdfmetrics.registerFont(TTFont(\"DejaVu\", font_path))\n\n    styles = getSampleStyleSheet()\n    custom_style = ParagraphStyle(\n        'CustomStyle',\n        parent=styles['Normal'],\n        fontName='DejaVu',\n        fontSize=12,\n    )\n\n    pdf_file_path = \"/kaggle/working/transcript.pdf\"\n    create_pdf(transcript_file_path, pdf_file_path)\n\n    return \"Аудиофайл успешно обработан. Транскрипция сохранена в transcript.txt, а PDF-файл в transcript.pdf\"\n\nbot.polling(none_stop=True, interval=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T11:24:26.671772Z","iopub.execute_input":"2024-01-23T11:24:26.672153Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\nInput #0, mp3, from '/kaggle/working/audio_input.ogg':\n  Duration: 00:04:18.36, start: 0.023021, bitrate: 91 kb/s\n  Stream #0:0: Audio: mp3, 48000 Hz, mono, fltp, 91 kb/s\n    Metadata:\n      encoder         : LAME3.100\nStream mapping:\n  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\nPress [q] to stop, [?] for help\nOutput #0, wav, to 'audio.wav':\n  Metadata:\n    ISFT            : Lavf58.76.100\n  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, mono, s16, 768 kb/s\n    Metadata:\n      encoder         : Lavc58.134.100 pcm_s16le\nsize=   24217kB time=00:04:18.31 bitrate= 768.0kbits/s speed= 733x    \nvideo:0kB audio:24217kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000315%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57c925075314c18adc24b071113bbd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2607890010574e318cd7d86847e68af3"}},"metadata":{}},{"name":"stderr","text":"\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =      33.09 ms /    58 runs   (    0.57 ms per token,  1752.74 tokens per second)\nllama_print_timings: prompt eval time =  463169.77 ms /  2134 tokens (  217.04 ms per token,     4.61 tokens per second)\nllama_print_timings:        eval time =   18972.32 ms /    57 runs   (  332.85 ms per token,     3.00 tokens per second)\nllama_print_timings:       total time =  483338.82 ms /  2191 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49d90467d994485295db0b490c94f29a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2768b1653ef747c08eda01d44cc02f54"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =       7.28 ms /    13 runs   (    0.56 ms per token,  1784.98 tokens per second)\nllama_print_timings: prompt eval time =  243370.66 ms /  1160 tokens (  209.80 ms per token,     4.77 tokens per second)\nllama_print_timings:        eval time =    4063.66 ms /    13 runs   (  312.59 ms per token,     3.20 tokens per second)\nllama_print_timings:       total time =  247973.73 ms /  1173 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2195a846e524c559603e2147f1f3974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4c18956b9f47c19132203d0482939b"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =      25.76 ms /    46 runs   (    0.56 ms per token,  1785.51 tokens per second)\nllama_print_timings: prompt eval time =  234056.36 ms /  1107 tokens (  211.43 ms per token,     4.73 tokens per second)\nllama_print_timings:        eval time =   14000.40 ms /    45 runs   (  311.12 ms per token,     3.21 tokens per second)\nllama_print_timings:       total time =  248740.11 ms /  1152 tokens\nffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\n[mp3 @ 0x596ce642a540] Estimating duration from bitrate, this may be inaccurate\nInput #0, mp3, from '/kaggle/working/audio_input.ogg':\n  Duration: 00:01:18.07, start: 0.000000, bitrate: 23 kb/s\n  Stream #0:0: Audio: mp3, 16000 Hz, mono, fltp, 24 kb/s\nStream mapping:\n  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\nPress [q] to stop, [?] for help\nOutput #0, wav, to 'audio.wav':\n  Metadata:\n    ISFT            : Lavf58.76.100\n  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n    Metadata:\n      encoder         : Lavc58.134.100 pcm_s16le\nsize=    2440kB time=00:01:18.04 bitrate= 256.1kbits/s speed= 835x    \nvideo:0kB audio:2440kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.003122%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b87832e4ff24209bba638d182b01f7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38f18a26fb6a4996b55e63fefd4f936f"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =      32.44 ms /    60 runs   (    0.54 ms per token,  1849.74 tokens per second)\nllama_print_timings: prompt eval time =  148238.90 ms /   707 tokens (  209.67 ms per token,     4.77 tokens per second)\nllama_print_timings:        eval time =   17971.58 ms /    59 runs   (  304.60 ms per token,     3.28 tokens per second)\nllama_print_timings:       total time =  166788.18 ms /   766 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e59609d0af7b45f29ff4c3b6a4d42adf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8261ab013a84f168ad443a5f5dbb77d"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =       5.55 ms /    11 runs   (    0.50 ms per token,  1981.62 tokens per second)\nllama_print_timings: prompt eval time =    4582.56 ms /    22 tokens (  208.30 ms per token,     4.80 tokens per second)\nllama_print_timings:        eval time =    2915.74 ms /    10 runs   (  291.57 ms per token,     3.43 tokens per second)\nllama_print_timings:       total time =    7555.27 ms /    32 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f555d5bd04d4856b2c3e298080df4bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f15f3db46e864e2eae5cbae51971a17d"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =       9.17 ms /    18 runs   (    0.51 ms per token,  1963.14 tokens per second)\nllama_print_timings: prompt eval time =    5205.39 ms /    23 tokens (  226.32 ms per token,     4.42 tokens per second)\nllama_print_timings:        eval time =    5105.41 ms /    17 runs   (  300.32 ms per token,     3.33 tokens per second)\nllama_print_timings:       total time =   10400.97 ms /    40 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffebce25ccd34b5ea2439941c4fee234"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f359bc0f554d8497af4ad1993006e6"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =       8.58 ms /    16 runs   (    0.54 ms per token,  1865.02 tokens per second)\nllama_print_timings: prompt eval time =    4915.64 ms /    24 tokens (  204.82 ms per token,     4.88 tokens per second)\nllama_print_timings:        eval time =    4797.07 ms /    16 runs   (  299.82 ms per token,     3.34 tokens per second)\nllama_print_timings:       total time =    9799.31 ms /    40 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be42c26505e454f9e78136b610c8f2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9117d7797df4dcda6eea73fddb218f3"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =       3.19 ms /     6 runs   (    0.53 ms per token,  1880.29 tokens per second)\nllama_print_timings: prompt eval time =    3799.06 ms /    16 tokens (  237.44 ms per token,     4.21 tokens per second)\nllama_print_timings:        eval time =    1511.08 ms /     5 runs   (  302.22 ms per token,     3.31 tokens per second)\nllama_print_timings:       total time =    5342.73 ms /    21 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd50c6399184a6f863d9de3086dc79b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3047accd8084675abe731b6c634783a"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =      23.50 ms /    43 runs   (    0.55 ms per token,  1829.79 tokens per second)\nllama_print_timings: prompt eval time =    4423.47 ms /    20 tokens (  221.17 ms per token,     4.52 tokens per second)\nllama_print_timings:        eval time =   12385.91 ms /    42 runs   (  294.90 ms per token,     3.39 tokens per second)\nllama_print_timings:       total time =   17016.73 ms /    62 tokens\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ac7e243e744f009c75692b6efedc11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a043072a04b410592353c734c3a2231"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1769.34 ms\nllama_print_timings:      sample time =      20.54 ms /    38 runs   (    0.54 ms per token,  1850.41 tokens per second)\nllama_print_timings: prompt eval time =    2541.56 ms /    12 tokens (  211.80 ms per token,     4.72 tokens per second)\nllama_print_timings:        eval time =   10958.33 ms /    37 runs   (  296.17 ms per token,     3.38 tokens per second)\nllama_print_timings:       total time =   13679.53 ms /    49 tokens\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}